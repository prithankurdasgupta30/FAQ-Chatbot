{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68381d06-aed0-4bf4-9a94-083040ada3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce61afbd-a736-47c2-9a96-f230798926d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Prompt', 'Response'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have never done programming in my life. Can ...</td>\n",
       "      <td>\"Yes, this is the perfect bootcamp for anyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why should I trust Codebasics?</td>\n",
       "      <td>Till now 9000 + learners have benefitted from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there any prerequisite for taking this boot...</td>\n",
       "      <td>\"Our bootcamp is specifically designed for beg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What datasets are used in this bootcamp? Is it...</td>\n",
       "      <td>The datasets used in this bootcamp are crafted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m not sure if this bootcamp is good enough f...</td>\n",
       "      <td>We got you covered. Go ahead and watch our you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt  \\\n",
       "0  I have never done programming in my life. Can ...   \n",
       "1                     Why should I trust Codebasics?   \n",
       "2  Is there any prerequisite for taking this boot...   \n",
       "3  What datasets are used in this bootcamp? Is it...   \n",
       "4  I’m not sure if this bootcamp is good enough f...   \n",
       "\n",
       "                                            Response  \n",
       "0  \"Yes, this is the perfect bootcamp for anyone ...  \n",
       "1  Till now 9000 + learners have benefitted from ...  \n",
       "2  \"Our bootcamp is specifically designed for beg...  \n",
       "3  The datasets used in this bootcamp are crafted...  \n",
       "4  We got you covered. Go ahead and watch our you...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"Dataset - Sheet2.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd080cf2-7ef5-4b26-bcf1-741925fb8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8383df15-734c-4dac-9098-311e3c78e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_texts = (\n",
    "    \"Question: \" + df[\"Prompt\"].astype(str) +\n",
    "    \"\\nAnswer: \" + df[\"Response\"].astype(str)\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2798f2da-5401-4817-b56d-6002bb234902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb85895326d34805b81592771d7415b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1100, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_embeddings = st_model.encode(\n",
    "    faq_texts,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "faq_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a03f00c-d9ed-4bec-bfe2-5c20eeed3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_faqs(query: str, top_k: int = 4):\n",
    "    \"\"\"Return top_k most similar FAQs to the query.\"\"\"\n",
    "    query_emb = st_model.encode([query], normalize_embeddings=True)\n",
    "    sims = cosine_similarity(query_emb, faq_embeddings)[0] \n",
    "    top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "    top_scores = sims[top_indices]\n",
    "    results = []\n",
    "    for idx, score in zip(top_indices, top_scores):\n",
    "        row = df.iloc[idx]\n",
    "        results.append({\n",
    "            \"index\": int(idx),\n",
    "            \"score\": float(score),\n",
    "            \"prompt\": row[\"Prompt\"],\n",
    "            \"response\": row[\"Response\"],\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f134162-b654-43b2-bfe6-082eed4dbc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.638\n",
      "FAQ Q: What exactly do you offer in terms of job guarantee?\n",
      "FAQ A: If you are worried about job guarantee, don't be. Our support team and mentors are available to help you understand the process step by step.\n",
      "------------------------------------------------------------\n",
      "Score: 0.615\n",
      "FAQ Q: What exactly do you offer in terms of job guarantee?\n",
      "FAQ A: The idea behind our job guarantee setup is to reduce friction for you. Wherever possible, we give multiple options and clear instructions so that you can focus on learning, not admin work.\n",
      "------------------------------------------------------------\n",
      "Score: 0.615\n",
      "FAQ Q: What exactly do you offer in terms of job guarantee?\n",
      "FAQ A: The idea behind our job guarantee setup is to reduce friction for you. Wherever possible, we give multiple options and clear instructions so that you can focus on learning, not admin work.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_q = \"Do you provide job assistance and job guarantee?\"\n",
    "for r in retrieve_similar_faqs(test_q, top_k=3):\n",
    "    print(f\"Score: {r['score']:.3f}\")\n",
    "    print(\"FAQ Q:\", r[\"prompt\"])\n",
    "    print(\"FAQ A:\", r[\"response\"])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a2bc8f-6eee-4d0d-b951-1054c8a0d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, top_k: int = 4, debug: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Return the best FAQ answer for the user's question.\n",
    "    Uses local embeddings + cosine similarity, no API calls.\n",
    "    \"\"\"\n",
    "    faqs = retrieve_similar_faqs(question, top_k=top_k)\n",
    "\n",
    "    if debug:\n",
    "        print(\"User Question:\", question)\n",
    "        print(\"\\nTop matches:\\n\")\n",
    "        for f in faqs:\n",
    "            print(f\"Score: {f['score']:.3f}\")\n",
    "            print(\"FAQ Q:\", f[\"prompt\"])\n",
    "            print(\"FAQ A:\", f[\"response\"])\n",
    "            print(\"-\" * 60)\n",
    "    best = faqs[0]\n",
    "    if best[\"score\"] < 0.4:\n",
    "        return \"I'm not sure based on the available FAQs.\"\n",
    "\n",
    "    return best[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91fad0f-aeed-484e-894d-216aa684ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  What are the assignments that we get?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: What are the assignments that we get?\n",
      "\n",
      "Top matches:\n",
      "\n",
      "Score: 0.687\n",
      "FAQ Q: What exactly do you offer in terms of assignment?\n",
      "FAQ A: Overall, you can think of our assignment design as simple, flexible, and learner-first. Once you enroll, you'll see that everything is documented and easy to follow.\n",
      "------------------------------------------------------------\n",
      "Score: 0.677\n",
      "FAQ Q: What exactly do you offer in terms of assignment?\n",
      "FAQ A: In short, our assignment system is built to support your learning journey, not get in the way. If you ever feel stuck, you can raise a ticket or ask in the community for quick help.\n",
      "------------------------------------------------------------\n",
      "Score: 0.669\n",
      "FAQ Q: What exactly do you offer in terms of assignment?\n",
      "FAQ A: We regularly review how our assignment is working in real life and update our processes so that learners like you have a smooth and predictable experience.\n",
      "------------------------------------------------------------\n",
      "Score: 0.659\n",
      "FAQ Q: I'm confused about the assignment. Could you give me more details?\n",
      "FAQ A: Overall, you can think of our assignment design as simple, flexible, and learner-first. Once you enroll, you'll see that everything is documented and easy to follow.\n",
      "------------------------------------------------------------\n",
      "A:  Overall, you can think of our assignment design as simple, flexible, and learner-first. Once you enroll, you'll see that everything is documented and easy to follow.\n"
     ]
    }
   ],
   "source": [
    "q = input(\"Enter your question: \")\n",
    "print(\"A: \", answer_question(q, top_k=4, debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b14ec-8f61-4636-b093-e6d78422eaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
